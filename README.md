一个入门的机器学习代码分享，包括：线性回归、逻辑回归、KNN算法、神经网络、决策树、集成学习（Bagging和AdaBoost）、贝叶斯算法、KMeans算法、PCA算法、SVM算法的复现，便于理解这些机器学习算法的实现逻辑。下面是对各算法的原理整理：
（1）逻辑回归
    逻辑回归假设数据服从伯努利分布（0-1分布），通过极大似然估计方法（量化模型预测结果与真实数据之间的匹配程度），使用梯度下降来求解参数，达到二分类目的的一个模型。
    逻辑回归模型由线性回归模型和sigmoid函数组成，使用sigmoid的好处是单调可微，并且很好的将预测值转化为接近0或1的数。
    逻辑回归损失函数：对数损失函数（根据实际意义-log）手推求导
![image](https://github.com/user-attachments/assets/a72a8814-5237-41b7-bcc9-ff1a44a77ac8)

（2）SVM
    SVM支持向量机是一种二分类模型，在特征空间上的间隔最大线性分类器。不允许样本错分就是硬间隔，允许错分就是软间隔。当靠近决策边界的点与决策边界的距离最大时是最好的分类选择。核函数解决特征在低维空间线性不可分的问题，通过核函数将低维映射到高维，实现线性可分。

（3）决策树（DT）
    决策树是一种基于树结构的监督学习算法，广泛应用于分类和回归任务。核心原理是对数据集进行递归划分，构建一棵树来模拟决策过程。
    决策树数据结构：
    ①根节点：包含整个数据集的起始节点。
    ②内部节点：根据特征进行划分的决策点。
    ③叶节点：最终的分类或回归结果。
    决策树构造方式：
    ①特征选择：选择基尼指数最小的特征对当前数据集进行划分，使划分后的子集尽可能“纯净”（同                 一类别的样本越多）
    基尼指数计算公式： ![image](https://github.com/user-attachments/assets/740830b3-e2f5-4ff0-9283-18f9e8068c03)
    ②停止条件：达到树最大深度 / 可分割样本数小于分割值 / 纯度达到要求（当前样本属同一类）
    ③剪枝：预剪枝和后剪枝

（4）集成学习Bagging
    Bagging是一种集成学习方法，旨在通过结合多个基学习器的预测结果来降低模型的方差，提高整体泛化能力，尤其适用于高方差、低偏差的模型。其核心思想是通过自助采样生成多个子数据集，并行训练多个基学习器，最终聚合它们的预测结果。
    Bagging的核心步骤：
    ①自助采样：从原始数据D中有放回地随机抽取m个样本，未抽中的是OOB样本（约36.8%）
    ②并行训练基学习器：对每个子数据集训练一个基学习器，所有基学习器独立训练，并行计算
    ③聚合：采用投票法 / 取平均

（5）AdaBoost提升树
    AdaBoost 是一种经典的Boosting集成学习算法，通过逐步调整样本权重和组合多个弱分类器，构建一个强分类器。每一轮训练后，增加分类错误样本的权重，减少正确分类样本的权重，迫使后续的弱分类器更关注之前分类错误的难样本，最终通过加权投票得到预测结果。
    AdaBoost的算法步骤：
    ①初始化：样本权重：![image](https://github.com/user-attachments/assets/030be206-527c-4199-9855-9e1ee2012e07)
（初始时所有样本权重相同）
    ②第一轮训练：所有样本权重相同，训练第一个弱分类器G1(x)。
        G1(x)分类器权重：![image](https://github.com/user-attachments/assets/3eab2f3e-c3eb-43f3-be16-bec632005584)
，其中ϵt是分类错误的比例
        分类错误的样本权重增加exp(αt)，分类正确的样本权重降低exp(-αt)
    ③第二轮训练：错误样本权重更高，新的分类器G2(x)会更关注这些样本。还会有分类错误的样本，      再计算G2(x)分类器权重，然后继续调整样本权重
    ④最终模型：组合所有弱分类器的预测，准确率高的分类器权重更大
    对所有分类器进行加权投票：![image](https://github.com/user-attachments/assets/610e407f-0099-4e72-971a-a1153bab1632)

（6）GBDT
    GBDT也叫梯度提升决策树，利用损失函数负梯度近似残差，采用加法模型，以及不断减小训练过程中的残差来提高预测精确度。通过多轮迭代，每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的残差的基础上进行训练。
    GBDT是回归树，不是分类树，但可以修改其损失函数为逻辑回归损失函数来进行分类任务。
通过基尼系数特征分裂收益计算重要性，通过记录特征的分裂总次数、总/平均收益来对重要性进行量化，被使用次数越多或带来增益越大的特征重要性越大。

（7）PCA算法（降维方法/主成分分析）
    PCA是一种无监督降维算法，其核心思想是通过线性变换将高维数据投影到低维空间，保留最大方差方向。
    核心目标：
        去相关：新特征（主成分）之间线性无关（协方差为0）
        最大化方差：优先保留数据方差最大的方向，减少信息损失
    降维步骤：
    ①数据标准化：![image](https://github.com/user-attachments/assets/7cd06b89-d49d-45d4-9065-329bae0c0f5e)
，μ为每列的均值，σ为标准差
    ②计算协方差矩阵：![image](https://github.com/user-attachments/assets/fffb98cc-e1ea-4f4f-913f-bbdcbdd3e9dc)
，X为m×n矩阵，m为样本数，n为特征数
    ③特征值分解：对协方差矩阵C进行特征分解![image](https://github.com/user-attachments/assets/7f8fff12-5ff5-4f30-a28f-959f6a6e522a)
    ④选择主成分：按特征值从大到小选择前k个特征向量，构成投影矩阵W（n×k）
    ⑤数据投影：将原始数据投影到低维空间Z=XW
    ⑥k的选择（前k个主成分保留95%方差）：![image](https://github.com/user-attachments/assets/b0fb840e-7e68-452e-97dd-a6c1f6b3b13e)

（8）后序算法继续补充，现在还没总结完！
